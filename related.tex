\section{Related Work}
\label{sRelated}

Domain adaptation has been extensively studied  in many areas, including in
statistics and machine learning \cite{shimodaira00shift,huang07correcting,bendavid07domain,pan2009survey},
speech and language
processing \cite{daume07easy,BlitzerEMNLP06Domain,LeggetterCSL95Maximum},
and more recently computer
vision \cite{bergamo09weak,gopalan2011domain,saenko2010adapting,kulisyou}.

Of particular relevance to our kernel methods  is the idea of learning feature representations that are domain-invariant, thus enabling transferring classifiers from the source domain to the target domain \cite{bendavid07domain,BlitzerEMNLP06Domain,BlitzerACL07domain,daume07easy,tca}.
The feature representation can be derived from using auxiliary tasks that predict ``pivot features''~\cite{ando05,BlitzerEMNLP06Domain}, augmenting the feature space~\cite{daume07easy,daume10co,li2012discriminative,gopalan2013learning}, co-training with multi-view representations~\cite{chen11cotrain}, or matching probabilistic distributions~\cite{tca}.
Those approaches are especially appealing to unsupervised domain adaptation as they  do not require labeled target data. %Other methods for unsupervised domain adaptation have been explored, for example, with transductive SVMs \cite{bergamo09weak} or iteratively relabeling (the target domain) \cite{bruzzone2010domain}.

Gopalan \emph{et al.}'s work is the closest to our GFK approach in spirit \cite{gopalan2011domain}. They have also explored the idea of using geodesic flows to derive intermediate subspaces that interpolate between the source and target domains. A crucial difference from ours is that they sample a \emph{finite} number of subspaces and stack these subspaces into a very high-dimensional projection matrix. As such, the dimension of their features needs to be reduced. This extra step, unfortunately, might introduce modeling errors and practical challenges. For instance, it is not clear how to choose the subspace sampling rate or the right feature dimension after reduction. %and whether the dimension reduction method used there necessarily helps classification.

In stark contrast, GFK is both conceptually and computationally simpler and eliminates the need to tune many hyper-parameters as by Gopalan \emph{et al.}'s approach. In particular, our kernel is in closed form, and computing it involves simple matrix algebra for example singular value decomposition. We have devised a fully automatic procedure, which is lacking in Gopalan \emph{et al.}'s work,  to choose the optimal dimensionality of the subspaces. {We also note that Zheng {\it et al.} proposed the same kernel, albeit independently and almost simultaneously~\cite{zheng2012grassmann}, though they did not offer an automatic procedure to determine the dimensions of the subspaces.}


While learning domain-invariant feature representations has been extensively studied in the literature, identifying and using instances that are distributed similarly to the target to bridge the two domains, as in our landmark-based approach,  has not been explored before.

The idea of auxiliary tasks was explored previously in \cite{BlitzerEMNLP06Domain} to identify invariant features. There, the tasks were to predict ``pivot features''. The derived features are PCA directions of those predictive models' parameters.  In our landmark-based approach,  however, the auxiliary tasks are new \emph{domain adaptation} tasks and the invariant features are learned \emph{discriminatively}. 

The procedure of moving labeled data between the source and the target domains in our landmark approach is similar in spirit to transductive style domain adaptation methods~\cite{bergamo09weak,bruzzone2010domain,chen11cotrain} where classifiers are iteratively retrained after labeled examples are moved.  Our approach has several crucial differences. First, we partition the source domain into two disjoint subsets, only once for each auxiliary task. We do not iteratively re-partition the data or iteratively re-train the classifiers.  Secondly, our primary goal is to learn useful features/kernels from each auxiliary task, while those methods  use the original features and aim to incrementally learn classifiers. Moreover, they depend very much on manually tuning several  parameters, which requires extensive computation of training and cross-validation.

Kernel mean matching has previously been used to \emph{weigh} samples from the source \cite{huang07correcting,tca,gretton09kmm} to correct the mismatch between domains. We \emph{select} samples as our landmarks. Those prior works typically do not yield sparse solutions (of the weights), and thus do not perform \emph{selection}.  Additionally, the inclusion of the balancing constraint in our formulation of eq.~(\ref{eSelected}) is crucial, as evidenced in our experimental studies (cf. Table~\ref{tResults} ). Without it, some classes could be underrepresented in selected landmarks, leading to poor performance.%%% due to unbalanced priors.

Broadly, our emphasis on learning kernels is relevant to the approaches of metric learning for domain adaptation, where the Mahalanobis metric  is a form of linear kernel functions~\cite{saenko2010adapting,kulisyou,shi2012}. Our GFK approach implicitly relies on the discriminative clustering assumption to reduce the discrepancy between how pairwise distances among data points are distributed in two domains. Such an assumption is explicitly exploited to derive an objective function to ensure adaptation~\cite{shi2012}. 

%The proposed method differentiates from those efforts by synthesizing auxiliary domain adaptation tasks that repartition data in the source and the target domains. This is different from using auxiliary classification tasks to model correlations between features \cite{BlitzerEMNLP06Domain}. Our approach is also different from  where the source and the target domains also change iteratively. In particular, our approach repartitions only once for each auxiliary task by pushing identified landmarks into the unlabeled target domain. Transuctive-style algorithms operate the opposite way, by revealing labels of unlabeled target data and making them part of the labeled set.  Finally, most of the previously mentioned work do not learn discriminatively to combine outputs of auxiliary tasks as we do.

%An important algorithmic property of our formulation differentiates our work from other existing works in applying MMD to \emph{weigh} samples from source domains \cite{huang07correcting,tca}. In particular, our formulation often leads to \textbf{sparse} solutions where some $\beta_m$ are zeroes. The nonzero $\beta_m$ correspond to landmarks.  Indeed, we recover the solution to the binary $\alpha_m$ by  finding the support of $\beta_m$, ie, $\alpha_m = \textsc{sign}(\beta_m)$. Due to differences in constraints and objective functions, other work do not aim to yield sparse solutions.
%Analysis with multiple scaling parameters has been previously studied, for example, in spectral clustering where Gaussian kernels have also been used to measure similarities between data \cite{zelnik2004self}. The motivation there is to choose a proper local scaling that is specific to each data point.  Our motivation is different: we compare two sets of data points and need to choose a proper ``global'' scaling so that points from two different domains could be identified as similar. %Nevertheless, it is interesting to notice that the hard problem of choosing the right scale can be attacked in the context of completely unsupervised clustering. In our case, however, we have labeled data to leverage. We describe our strategy after we explain how to setup supervised learning problems.



